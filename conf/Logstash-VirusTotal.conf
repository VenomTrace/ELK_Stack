input {
  beats {
    port => 5045
  }
}

filter {
  grok {
    match => {
      "message" => "%{MONTHNUM:month}/%{MONTHDAY:day}/%{YEAR:year} %{TIME:log_time} %{WORD:Time_Form} %{WORD:thread_id} %{WORD:packet_type}  %{NOTSPACE:packet_id} %{WORD:protocol} %{WORD:action} %{IP:client_ip} +%{WORD:port} +%{GREEDYDATA:Response_Query} \[%{NUMBER:FlagNumber}\s+(%{WORD:Authoritative_Answer})?\s+(%{WORD:Recursion_Desired})?\s+(%{WORD:Truncated_Response})?\s+(%{WORD:Error_Check}\s*)?\] %{WORD:Record_Type} +%{GREEDYDATA:url}"
    }
  }

  # Transform query_name into a readable domain
  ruby {
    code =>"
      require 'uri'
      require 'net/http'
      require 'json'  # Load the JSON module for parsing

      # URL Transformation: Remove unwanted parts of the URL
      if event.get('url') && !event.get('url').empty?
        url = event.get('url')
        url = url.gsub(/\(\d+\)/, '.')
        url = url.sub(/\.$/, '')
        url = url.sub(/^./, '')
        event.set('url', url)
      end

      # Domain from the Logstash event
      domain = event.get('url')
      if domain && !domain.empty?
        # Construct the URL for VirusTotal API request
        url = URI('https://www.virustotal.com/api/v3/domains/' + url)

        # Initialize HTTP request
        http = Net::HTTP.new(url.host, url.port)
        http.use_ssl = true

        # Set the API key and necessary headers
        request = Net::HTTP::Get.new(url)
        request['X-Apikey'] = '4b162a355087b6b98fc6c9cb30e16712d32e3496f86bcb5ebe31b8692b87a055'  # Replace with your API key
        request['accept'] = 'application/json'

        # Send the request and get the response
        response = http.request(request)

        # Parse the JSON response
        json_response = JSON.parse(response.read_body)

        # Check if the 'data' and 'attributes' fields exist
        if json_response['data'] && json_response['data']['attributes'] && json_response['data']['attributes']['last_analysis_stats']
          # Extract the specific fields
          analysis_stats = json_response['data']['attributes']['last_analysis_stats']

          # Set the extracted fields into Logstash event
          event.set('malicious', analysis_stats['malicious'])
          event.set('suspicious', analysis_stats['suspicious'])
          event.set('undetected', analysis_stats['undetected'])
          event.set('harmless', analysis_stats['harmless'])
          event.set('timeout', analysis_stats['timeout'])
        else
          # If the fields are not available, set them to null or a default value
          event.set('malicious', nil)
          event.set('suspicious', nil)
          event.set('undetected', nil)
          event.set('harmless', nil)
          event.set('timeout', nil)
        end
      end
    "
  }

  geoip {
    source => "client_ip"  # Field to perform the GeoIP lookup on
    target => "location"   # Target field to store the GeoIP information
  }

  # Optionally, create a timestamp field by combining log_date and log_time
  date {
    match => ["log_date log_time", "MM/dd/yyyy hh:mm:ss a"]
    target => "@timestamp"
  }
}

output {
  elasticsearch {
    hosts => ["http://10.10.10.16:9200"]
    index => "venom-dns-%{+YYYY.MM.dd}"
  }

  stdout {
    codec => rubydebug
  }
}
